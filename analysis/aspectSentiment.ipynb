{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def extract_aspect_sentiment_pairs(text):\n",
    "    blob = TextBlob(text)\n",
    "    aspect_sentiment_pairs = []\n",
    "    for sentence in blob.sentences:\n",
    "        for noun_phrase in sentence.noun_phrases:\n",
    "            aspect = noun_phrase\n",
    "            sentiment = sentence.sentiment.polarity\n",
    "            aspect_sentiment_pairs.append((aspect, sentiment))\n",
    "    return aspect_sentiment_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "'This chocolate truffle cake is really tasty',\n",
    "'This party is amazing!',\n",
    "'My mom is the best!',\n",
    "'App response is very slow!'\n",
    "'The trip to India was very enjoyable'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate truffle cake', 0.2)]\n",
      "[]\n",
      "[]\n",
      "[('app', 0.42500000000000004), ('india', 0.42500000000000004)]\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    result = extract_aspect_sentiment_pairs(sent)\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting stanfordnlp\n",
      "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
      "                                              0.0/158.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 158.8/158.8 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from stanfordnlp) (1.24.3)\n",
      "Collecting protobuf (from stanfordnlp)\n",
      "  Using cached protobuf-4.23.2-cp310-abi3-win_amd64.whl (422 kB)\n",
      "Requirement already satisfied: requests in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from stanfordnlp) (2.31.0)\n",
      "Collecting torch>=1.0.0 (from stanfordnlp)\n",
      "  Using cached torch-2.0.1-cp310-cp310-win_amd64.whl (172.3 MB)\n",
      "Requirement already satisfied: tqdm in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from stanfordnlp) (4.65.0)\n",
      "Collecting filelock (from torch>=1.0.0->stanfordnlp)\n",
      "  Downloading filelock-3.12.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (4.6.3)\n",
      "Collecting sympy (from torch>=1.0.0->stanfordnlp)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from requests->stanfordnlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from requests->stanfordnlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from requests->stanfordnlp) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from requests->stanfordnlp) (2023.5.7)\n",
      "Requirement already satisfied: colorama in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from tqdm->stanfordnlp) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\personal-projects\\ml-projects\\smta\\smteenv\\lib\\site-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.0.0->stanfordnlp)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, protobuf, filelock, torch, stanfordnlp\n",
      "Successfully installed filelock-3.12.1 mpmath-1.3.0 protobuf-4.23.2 stanfordnlp-0.2.0 sympy-1.12 torch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "\n",
      "Default download directory: C:\\Users\\mohammedshuaib\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\mohammedshuaib\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235M/235M [00:42<00:00, 5.50MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\mohammedshuaib\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The Sound Quality is great but the battery life is very bad.\"\n",
    "txt = txt.lower()\n",
    "sentList = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for line in sentList:\n",
    "    txt_list = nltk.word_tokenize(line)\n",
    "    taggedList = nltk.pos_tag(txt_list)\n",
    "print(taggedList)\n",
    "#Output: [('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the soundquality is great but the batterylife is very bad .\n"
     ]
    }
   ],
   "source": [
    "newwordList = []\n",
    "flag = 0\n",
    "for i in range(0,len(taggedList)-1):\n",
    "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "        newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "        flag=1\n",
    "    else:\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "        newwordList.append(taggedList[i][0])\n",
    "        if(i==len(taggedList)-2):\n",
    "            newwordList.append(taggedList[i+1][0])\n",
    "finaltxt = ' '.join(word for word in newwordList)\n",
    "print(finaltxt)\n",
    "#Output: the soundquality is great but the batterylife is very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "taggedList = nltk.pos_tag(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\mohammedshuaib\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index_select(): Expected dtype int32 or int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nlp \u001b[39m=\u001b[39m stanfordnlp\u001b[39m.\u001b[39mPipeline()\n\u001b[1;32m----> 2\u001b[0m doc \u001b[39m=\u001b[39m nlp(finaltxt)\n\u001b[0;32m      3\u001b[0m dep_node \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m dep_edge \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msentences[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdependencies:\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\pipeline\\core.py:176\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(doc, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    175\u001b[0m     doc \u001b[39m=\u001b[39m Document(doc)\n\u001b[1;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc)\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\pipeline\\core.py:170\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mfor\u001b[39;00m processor_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor_names:\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessors[processor_name]\u001b[39m.\u001b[39;49mprocess(doc)\n\u001b[0;32m    171\u001b[0m doc\u001b[39m.\u001b[39mload_annotations()\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\pipeline\\lemma_processor.py:66\u001b[0m, in \u001b[0;36mLemmaProcessor.process\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     64\u001b[0m edits \u001b[39m=\u001b[39m []\n\u001b[0;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m i, b \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(seq2seq_batch):\n\u001b[1;32m---> 66\u001b[0m     ps, es \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mpredict(b, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mbeam_size\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     67\u001b[0m     preds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ps\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m es \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\models\\lemma\\trainer.py:88\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, batch, beam_size)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m     87\u001b[0m batch_size \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m preds, edit_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(src, src_mask, pos\u001b[39m=\u001b[39;49mpos, beam_size\u001b[39m=\u001b[39;49mbeam_size)\n\u001b[0;32m     89\u001b[0m pred_seqs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab[\u001b[39m'\u001b[39m\u001b[39mchar\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munmap(ids) \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m preds] \u001b[39m# unmap to tokens\u001b[39;00m\n\u001b[0;32m     90\u001b[0m pred_seqs \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mprune_decoded_seqs(pred_seqs)\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\models\\common\\seq2seq_model.py:210\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[1;34m(self, src, src_mask, pos, beam_size)\u001b[0m\n\u001b[0;32m    208\u001b[0m         done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [b]\n\u001b[0;32m    209\u001b[0m     \u001b[39m# update beam state\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     update_state((hn, cn), b, beam[b]\u001b[39m.\u001b[39;49mget_current_origin(), beam_size)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(done) \u001b[39m==\u001b[39m batch_size:\n\u001b[0;32m    213\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Personal-Projects\\ML-Projects\\SMTA\\smteEnv\\lib\\site-packages\\stanfordnlp\\models\\common\\seq2seq_model.py:193\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict.<locals>.update_state\u001b[1;34m(states, idx, positions, beam_size)\u001b[0m\n\u001b[0;32m    191\u001b[0m br, d \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39msize()\n\u001b[0;32m    192\u001b[0m s \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(beam_size, br \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m beam_size, d)[:,idx]\n\u001b[1;32m--> 193\u001b[0m s\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcopy_(s\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, positions))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index_select(): Expected dtype int32 or int64 for index"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "doc = nlp(finaltxt)\n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "for i in range(0, len(dep_node)):\n",
    "    if (int(dep_node[i][1]) != 0):\n",
    "        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "print(dep_node)\n",
    "#Output: [['the', 'soundquality', 'det'], ['soundquality', 'great', 'nsubj'], ['is', 'great', 'cop'], ['great', '0', 'root'], ['but', 'bad', 'cc'], ['the', 'batterylife', 'det'], ['batterylife', 'bad', 'nsubj'], ['is', 'bad', 'cop'], ['very', 'bad', 'advmod'], ['bad', 'great', 'conj'], ['.', 'great', 'punct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList = []\n",
    "categories = []\n",
    "for i in taggedList:\n",
    "    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "        featureList.append(list(i))\n",
    "        totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "        categories.append(i[0])\n",
    "print(featureList)\n",
    "print(categoriesList)\n",
    "#Output: [['soundquality', 'NN'], ['great', 'JJ'], ['batterylife', 'NN'], ['bad', 'JJ']]\n",
    "#Output: ['soundquality', 'great', 'batterylife', 'bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcluster = []\n",
    "for i in featureList:\n",
    "    filist = []\n",
    "    for j in dep_node:\n",
    "        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "            if(j[0]==i[0]):\n",
    "                filist.append(j[1])\n",
    "            else:\n",
    "                filist.append(j[0])\n",
    "    fcluster.append([i[0], filist])\n",
    "print(fcluster)\n",
    "#Output: [['soundquality', ['great']], ['great', ['soundquality']], ['batterylife', ['bad']], ['bad', ['batterylife', 'very']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalcluster = []\n",
    "dic = {}\n",
    "for i in featureList:\n",
    "    dic[i[0]] = i[1]\n",
    "for i in fcluster:\n",
    "    if(dic[i[0]]==\"NN\"):\n",
    "        finalcluster.append(i)\n",
    "print(finalcluster)\n",
    "#Output: [['soundquality', ['great']], ['batterylife', ['bad']]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smteEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
